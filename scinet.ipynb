{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from scinet import SciNet\n",
    "import pandas as pd\n",
    "from scinet_utils import target_loss \n",
    "from loader import build_dataloader\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from scinet import SciNet\n",
    "from scinet_utils import target_loss\n",
    "from loader import build_dataloader\n",
    "\n",
    "def generate_data(size, size_, t_max):\n",
    "    t = np.linspace(0, t_max, size)\n",
    "    min_fr, max_fr = 0.01, 100\n",
    "    fr = np.linspace(min_fr, max_fr, size_)\n",
    "    start_st, end_st = 0.01, 100\n",
    "    st = np.logspace(np.log10(start_st), np.log10(end_st), size_, endpoint=True)\n",
    "\n",
    "    def f(t, st, fr):\n",
    "        return st**2 * fr * (1 - t/st - np.exp(-t/st))\n",
    "\n",
    "    data = []\n",
    "    for st_ in st:\n",
    "        for fr_ in fr:\n",
    "            example = list(f(t, st_, fr_))\n",
    "            t_pred = np.random.uniform(0, t_max)\n",
    "            pred = f(t_pred, st_, fr_)\n",
    "            example.extend([fr_, st_, t_pred, pred])\n",
    "            data.append(example)\n",
    "\n",
    "    columns = [str(i) for i in range(size)]\n",
    "    columns.extend([\"fr\", \"st\", \"t_pred\", \"pred\"])\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "    return df\n",
    "\n",
    "def train_sci_net(scinet, dataloader, optimizer, scheduler, beta, N_EPOCHS, device):\n",
    "    hist_error = []\n",
    "    hist_kl = []\n",
    "    hist_loss = []\n",
    "\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        epoch_error = []\n",
    "        epoch_kl = []\n",
    "        epoch_loss = []\n",
    "        for minibatch in dataloader:\n",
    "            time_series, fr, st, question, answer = (\n",
    "                minibatch['time_series'].to(device) / 5,\n",
    "                minibatch['fr'].to(device) / 5,\n",
    "                minibatch['st'].to(device) / 5,\n",
    "                minibatch['question'].to(device) / 5,\n",
    "                minibatch['answer'].to(device) / 5\n",
    "            )\n",
    "            inputs = torch.cat((time_series, question.view(-1, 1)), 1)\n",
    "            outputs = answer\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            pred = scinet.forward(inputs)\n",
    "            loss_ = target_loss(pred, outputs)\n",
    "            kl = beta * scinet.kl_loss\n",
    "            loss = loss_ + kl\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            error = torch.mean(torch.sqrt((pred[:, 0] - outputs)**2)).detach().cpu().numpy()\n",
    "            epoch_error.append(float(error))\n",
    "            epoch_kl.append(float(kl.data.detach().cpu().numpy()))\n",
    "            epoch_loss.append(float(loss_.data.detach().cpu().numpy()))\n",
    "\n",
    "        hist_error.append(np.mean(epoch_error))\n",
    "        hist_loss.append(np.mean(epoch_loss))\n",
    "        hist_kl.append(np.mean(epoch_kl))\n",
    "\n",
    "        before_lr = optimizer.param_groups[0][\"lr\"]\n",
    "        scheduler.step()\n",
    "        after_lr = optimizer.param_groups[0][\"lr\"]\n",
    "        print(\"Epoch %d: SGD lr %.6f -> %.6f\" % (epoch+1, before_lr, after_lr))\n",
    "        print(\"Epoch %d -- loss %f, RMS error %f, KL %f\" % (epoch+1, hist_loss[-1], hist_error[-1], hist_kl[-1]))\n",
    "\n",
    "    return hist_error, hist_kl, hist_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: SGD lr 0.001000 -> 0.000980\n",
      "Epoch 1 -- loss 104225.428040, RMS error 14.993181, KL 553.460531\n",
      "Epoch 2: SGD lr 0.000980 -> 0.000960\n",
      "Epoch 2 -- loss 96632.580091, RMS error 15.917623, KL 208.401646\n",
      "Epoch 3: SGD lr 0.000960 -> 0.000941\n",
      "Epoch 3 -- loss 60980.338022, RMS error 13.107974, KL 1563.974118\n",
      "Epoch 4: SGD lr 0.000941 -> 0.000921\n",
      "Epoch 4 -- loss 15271.538676, RMS error 6.248935, KL 535.794238\n",
      "Epoch 5: SGD lr 0.000921 -> 0.000901\n",
      "Epoch 5 -- loss 6755.330499, RMS error 4.092488, KL 412.686538\n",
      "Epoch 6: SGD lr 0.000901 -> 0.000881\n",
      "Epoch 6 -- loss 3745.837977, RMS error 3.006481, KL 347.543879\n",
      "Epoch 7: SGD lr 0.000881 -> 0.000861\n",
      "Epoch 7 -- loss 2141.860327, RMS error 2.228073, KL 299.291478\n",
      "Epoch 8: SGD lr 0.000861 -> 0.000841\n",
      "Epoch 8 -- loss 1297.927104, RMS error 1.732393, KL 266.215634\n",
      "Epoch 9: SGD lr 0.000841 -> 0.000822\n",
      "Epoch 9 -- loss 840.943469, RMS error 1.422136, KL 247.214320\n",
      "Epoch 10: SGD lr 0.000822 -> 0.000802\n",
      "Epoch 10 -- loss 619.780625, RMS error 1.239820, KL 228.418946\n",
      "Epoch 11: SGD lr 0.000802 -> 0.000782\n",
      "Epoch 11 -- loss 442.812018, RMS error 1.065253, KL 212.932819\n",
      "Epoch 12: SGD lr 0.000782 -> 0.000762\n",
      "Epoch 12 -- loss 368.888970, RMS error 0.991105, KL 199.252387\n",
      "Epoch 13: SGD lr 0.000762 -> 0.000742\n",
      "Epoch 13 -- loss 291.080580, RMS error 0.887345, KL 186.946517\n",
      "Epoch 14: SGD lr 0.000742 -> 0.000723\n",
      "Epoch 14 -- loss 238.510628, RMS error 0.808774, KL 176.901216\n",
      "Epoch 15: SGD lr 0.000723 -> 0.000703\n",
      "Epoch 15 -- loss 193.132731, RMS error 0.754687, KL 166.870945\n",
      "Epoch 16: SGD lr 0.000703 -> 0.000683\n",
      "Epoch 16 -- loss 173.508904, RMS error 0.713864, KL 158.891470\n",
      "Epoch 17: SGD lr 0.000683 -> 0.000663\n",
      "Epoch 17 -- loss 140.543107, RMS error 0.651981, KL 150.692570\n",
      "Epoch 18: SGD lr 0.000663 -> 0.000643\n",
      "Epoch 18 -- loss 133.095482, RMS error 0.639852, KL 142.582274\n",
      "Epoch 19: SGD lr 0.000643 -> 0.000623\n",
      "Epoch 19 -- loss 130.124561, RMS error 0.627035, KL 135.315396\n",
      "Epoch 20: SGD lr 0.000623 -> 0.000604\n",
      "Epoch 20 -- loss 108.184144, RMS error 0.568671, KL 128.619376\n",
      "Epoch 21: SGD lr 0.000604 -> 0.000584\n",
      "Epoch 21 -- loss 100.288575, RMS error 0.542050, KL 123.030143\n",
      "Epoch 22: SGD lr 0.000584 -> 0.000564\n",
      "Epoch 22 -- loss 79.195127, RMS error 0.486598, KL 117.292502\n",
      "Epoch 23: SGD lr 0.000564 -> 0.000544\n",
      "Epoch 23 -- loss 72.116780, RMS error 0.468765, KL 112.462197\n",
      "Epoch 24: SGD lr 0.000544 -> 0.000524\n",
      "Epoch 24 -- loss 60.197772, RMS error 0.430984, KL 107.546219\n",
      "Epoch 25: SGD lr 0.000524 -> 0.000504\n",
      "Epoch 25 -- loss 59.666390, RMS error 0.430208, KL 103.152759\n",
      "Epoch 26: SGD lr 0.000504 -> 0.000485\n",
      "Epoch 26 -- loss 60.708681, RMS error 0.426328, KL 98.766382\n",
      "Epoch 27: SGD lr 0.000485 -> 0.000465\n",
      "Epoch 27 -- loss 46.774661, RMS error 0.380768, KL 95.014416\n",
      "Epoch 28: SGD lr 0.000465 -> 0.000445\n",
      "Epoch 28 -- loss 42.782790, RMS error 0.366058, KL 91.287705\n",
      "Epoch 29: SGD lr 0.000445 -> 0.000425\n",
      "Epoch 29 -- loss 38.463286, RMS error 0.346191, KL 87.769276\n",
      "Epoch 30: SGD lr 0.000425 -> 0.000405\n",
      "Epoch 30 -- loss 37.921544, RMS error 0.341837, KL 84.830147\n",
      "Epoch 31: SGD lr 0.000405 -> 0.000386\n",
      "Epoch 31 -- loss 30.722677, RMS error 0.309683, KL 82.048910\n",
      "Epoch 32: SGD lr 0.000386 -> 0.000366\n",
      "Epoch 32 -- loss 28.538172, RMS error 0.299647, KL 79.280626\n",
      "Epoch 33: SGD lr 0.000366 -> 0.000346\n",
      "Epoch 33 -- loss 27.717658, RMS error 0.295481, KL 77.219086\n",
      "Epoch 34: SGD lr 0.000346 -> 0.000326\n",
      "Epoch 34 -- loss 26.281601, RMS error 0.287997, KL 75.064484\n",
      "Epoch 35: SGD lr 0.000326 -> 0.000306\n",
      "Epoch 35 -- loss 25.987716, RMS error 0.285305, KL 73.323421\n",
      "Epoch 36: SGD lr 0.000306 -> 0.000286\n",
      "Epoch 36 -- loss 26.569961, RMS error 0.292194, KL 71.870676\n",
      "Epoch 37: SGD lr 0.000286 -> 0.000267\n",
      "Epoch 37 -- loss 18.551408, RMS error 0.245690, KL 69.938527\n",
      "Epoch 38: SGD lr 0.000267 -> 0.000247\n",
      "Epoch 38 -- loss 17.247707, RMS error 0.240186, KL 68.299323\n",
      "Epoch 39: SGD lr 0.000247 -> 0.000227\n",
      "Epoch 39 -- loss 17.160458, RMS error 0.237255, KL 66.985446\n",
      "Epoch 40: SGD lr 0.000227 -> 0.000207\n",
      "Epoch 40 -- loss 16.421462, RMS error 0.234551, KL 65.452502\n",
      "Epoch 41: SGD lr 0.000207 -> 0.000187\n",
      "Epoch 41 -- loss 14.815705, RMS error 0.224909, KL 64.344189\n",
      "Epoch 42: SGD lr 0.000187 -> 0.000168\n",
      "Epoch 42 -- loss 15.511237, RMS error 0.226155, KL 63.261890\n",
      "Epoch 43: SGD lr 0.000168 -> 0.000148\n",
      "Epoch 43 -- loss 15.209591, RMS error 0.224085, KL 62.253995\n",
      "Epoch 44: SGD lr 0.000148 -> 0.000128\n",
      "Epoch 44 -- loss 15.332418, RMS error 0.226977, KL 61.525726\n",
      "Epoch 45: SGD lr 0.000128 -> 0.000108\n",
      "Epoch 45 -- loss 11.565258, RMS error 0.200536, KL 60.719021\n",
      "Epoch 46: SGD lr 0.000108 -> 0.000088\n",
      "Epoch 46 -- loss 11.611263, RMS error 0.199829, KL 60.005053\n",
      "Epoch 47: SGD lr 0.000088 -> 0.000068\n",
      "Epoch 47 -- loss 10.814907, RMS error 0.193343, KL 59.479302\n",
      "Epoch 48: SGD lr 0.000068 -> 0.000049\n",
      "Epoch 48 -- loss 10.467091, RMS error 0.189836, KL 58.991234\n",
      "Epoch 49: SGD lr 0.000049 -> 0.000029\n",
      "Epoch 49 -- loss 9.649099, RMS error 0.182611, KL 58.634250\n",
      "Epoch 50: SGD lr 0.000029 -> 0.000009\n",
      "Epoch 50 -- loss 9.321444, RMS error 0.180117, KL 58.317490\n",
      "Model saved to saved_models/scinet1-25epoch50.dat\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "sizes = [25]\n",
    "N_EPOCHS = 50\n",
    "size_ = 200\n",
    "t_max = 5\n",
    "data_file = \"data.csv\"\n",
    "for size in sizes: \n",
    "    df = generate_data(size, size_, t_max)\n",
    "    df.to_csv(data_file)\n",
    "\n",
    "    scinet = SciNet(size, 1, 2, 100).to(device)  # Move the model to the GPU\n",
    "    dataloader = build_dataloader(size=size, batch_size=128)\n",
    "\n",
    "    SAVE_PATH = f\"saved_models/scinet1-{size}epoch{N_EPOCHS}.dat\"\n",
    "    optimizer = optim.Adam(scinet.parameters(), lr=0.001)\n",
    "    beta = 0.5\n",
    "    scheduler = lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.009, total_iters=N_EPOCHS)\n",
    "\n",
    "    hist_error = []\n",
    "    hist_kl = []\n",
    "    hist_loss = []\n",
    "\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        epoch_error = []\n",
    "        epoch_kl = []\n",
    "        epoch_loss = []\n",
    "        for minibatch in dataloader:\n",
    "            time_series, fr, st, question, answer = (\n",
    "                minibatch['time_series'].to(device) / 5,\n",
    "                minibatch['fr'].to(device) / 5,\n",
    "                minibatch['st'].to(device) / 5,\n",
    "                minibatch['question'].to(device) / 5,\n",
    "                minibatch['answer'].to(device) / 5\n",
    "            )\n",
    "            inputs = torch.cat((time_series, question.view(-1, 1)), 1)\n",
    "            outputs = answer\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            input, pred = scinet.forward(inputs)\n",
    "            loss_ = target_loss(pred, outputs)\n",
    "            kl = beta * scinet.kl_loss\n",
    "            loss = loss_ + kl\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            error = torch.mean(torch.sqrt((pred[:, 0] - outputs)**2)).detach().cpu().numpy()\n",
    "            epoch_error.append(float(error))\n",
    "            epoch_kl.append(float(kl.data.detach().cpu().numpy()))\n",
    "            epoch_loss.append(float(loss_.data.detach().cpu().numpy()))\n",
    "\n",
    "        hist_error.append(np.mean(epoch_error))\n",
    "        hist_loss.append(np.mean(epoch_loss))\n",
    "        hist_kl.append(np.mean(epoch_kl))\n",
    "\n",
    "        before_lr = optimizer.param_groups[0][\"lr\"]\n",
    "        scheduler.step()\n",
    "        after_lr = optimizer.param_groups[0][\"lr\"]\n",
    "        print(\"Epoch %d: SGD lr %.6f -> %.6f\" % (epoch+1, before_lr, after_lr))\n",
    "        print(\"Epoch %d -- loss %f, RMS error %f, KL %f\" % (epoch+1, hist_loss[-1], hist_error[-1], hist_kl[-1]))\n",
    "\n",
    "\n",
    "    torch.save(scinet.state_dict(), SAVE_PATH)\n",
    "    print(f\"Model saved to {SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torch.Tensor().to(device)\n",
    "for minibatch in dataloader:\n",
    "    time_series, fr, st, question, answer = (\n",
    "        minibatch['time_series'].to(device) / 5,\n",
    "        minibatch['fr'].to(device) / 5,\n",
    "        minibatch['st'].to(device) / 5,\n",
    "        minibatch['question'].to(device) / 5,\n",
    "        minibatch['answer'].to(device) / 5\n",
    "    )\n",
    "    inputs = torch.cat((time_series, question.view(-1, 1)), 1)\n",
    "    outputs = answer\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    input, pred = scinet.forward(inputs)\n",
    "    data = torch.cat((input, pred), 1)\n",
    "    error = torch.mean(torch.sqrt((pred[:, 0] - outputs)**2)).detach().cpu().numpy()\n",
    "    dataset = torch.cat((dataset, data), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dataset.detach().cpu().numpy())\n",
    "df.to_csv(\"scinet_output.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
